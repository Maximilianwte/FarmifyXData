{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper_Yellowpages:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://www.yellowpages.com.au/search/listings?clue=farm&locationClue=All+States&lat=&lon=&referredBy=UNKNOWN&selectedViewMode=list&eventType=refinement&openNow=false&refinedCategory=35823\"\n",
    "        self.driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n",
    "        \"\"\"options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"headless\")\n",
    "        self.driver = webdriver.Chrome(chrome_options=options)\n",
    "        self.driver = webdriver.Chrome()\"\"\"\n",
    "    def scrape_page(self, url = \"\"):\n",
    "        driver = self.driver\n",
    "        if url != \"\":\n",
    "            driver.get(url)\n",
    "        else:\n",
    "            url = driver.current_url\n",
    "\n",
    "        results = driver.find_elements_by_class_name('listing')\n",
    "\n",
    "        columns = [\"Name\", \"Adress\", \"Phone\", \"Website\", \"Category\", \"Email\", \"Source\", \"DateUpdated\", \"DateChecked\"]\n",
    "        webObjects = [\"listing-name\", \"listing-address\", \"contact-text\", \"contact-url\", \"listing-heading\", \"contact-email\", 0, 0]\n",
    "        dataDict = {\"Name\": [], \"Adress\": [], \"GeoLocation\": [], \"Phone\": [], \"Website\": [], \"Category\": [], \"Email\": [], \"Crops\": [], \"Source\": [], \"DateUpdated\": [], \"DateChecked\": []}\n",
    "\n",
    "        for result in results:\n",
    "            for i in range(0, len(columns)):\n",
    "                if columns[i] == \"DateUpdated\":\n",
    "                    dataDict[columns[i]].append(date.today().strftime(\"%d/%m/%Y\"))\n",
    "                    dataDict[\"GeoLocation\"].append(\"\")\n",
    "                    dataDict[\"Crops\"].append(\"\")\n",
    "                elif columns[i] == \"Source\":\n",
    "                    dataDict[columns[i]].append(url)\n",
    "                elif columns[i] == \"DateChecked\":\n",
    "                    dataDict[columns[i]].append(\"-\")\n",
    "                elif columns[i] == \"Phone\":\n",
    "                    try:\n",
    "                        value = result.find_element_by_class_name(webObjects[i])\n",
    "                        dataDict[columns[i]].append(value.text)\n",
    "                    except:\n",
    "                        dataDict[columns[i]].append(\"N/V\")\n",
    "                elif columns[i] == \"Email\":\n",
    "                    try:\n",
    "                        value = result.find_element_by_class_name(webObjects[i])\n",
    "                        dataDict[columns[i]].append(value.get_attribute(\"data-email\"))\n",
    "                    except:\n",
    "                        dataDict[columns[i]].append(\"N/V\")\n",
    "                elif columns[i] == \"Website\":\n",
    "                    try:\n",
    "                        value = result.find_element_by_class_name(webObjects[i])\n",
    "                        dataDict[columns[i]].append(value.get_attribute(\"href\"))\n",
    "                    except:\n",
    "                        dataDict[columns[i]].append(\"N/V\")\n",
    "                elif columns[i] == \"Category\":\n",
    "                    try:\n",
    "                        value = result.find_element_by_class_name(webObjects[i])\n",
    "                        dataDict[columns[i]].append(value.text)\n",
    "                    except:\n",
    "                        dataDict[columns[i]].append(\"N/V\")\n",
    "                elif webObjects[i] != 0:    \n",
    "                    try:\n",
    "                        value = result.find_element_by_class_name(webObjects[i])\n",
    "                        dataDict[columns[i]].append(value.text)\n",
    "                    except:\n",
    "                        dataDict[columns[i]].append(\"N/V\") \n",
    "                else:\n",
    "                    dataDict[columns[i]].append(\"N/V\") \n",
    "\n",
    "        df = pd.DataFrame(dataDict)\n",
    "        return df\n",
    "\n",
    "    def scrape_all_pages(self, url):\n",
    "        driver = self.driver\n",
    "        driver.get(url)\n",
    "        \n",
    "        i = 0\n",
    "        while i < 30:\n",
    "            df_page = self.scrape_page()\n",
    "\n",
    "            if i != 0:\n",
    "                df = pd.concat([df, df_page], sort=False)\n",
    "            else:\n",
    "                df = df_page\n",
    "\n",
    "            try:\n",
    "                driver.find_element_by_xpath(\"//a[contains(text(),'Next')]\").click()\n",
    "            except:\n",
    "                break\n",
    "            i = i + 1\n",
    "        return df\n",
    "     \n",
    "    def scrape_multiple_searches(self, searches):\n",
    "        index = 0\n",
    "        for title in searches:\n",
    "            for state in [\"NSW\", \"QLD\", \"VIC\", \"WA\", \"SA\", \"TAS\", \"ACT\", \"NT\"]:                \n",
    "                url = f\"https://www.yellowpages.com.au/search/listings?clue={title}&locationClue={state}&pageNumber=1&referredBy=www.yellowpages.com.au&&eventType=pagination\"\n",
    "\n",
    "                df_page = self.scrape_all_pages(url)\n",
    "                if index == 0:  \n",
    "                    df = df_page\n",
    "                else:\n",
    "                    df = pd.concat([df, df_page], sort=False)\n",
    "                \n",
    "                if index % 32 == 0:\n",
    "                    df = df.drop_duplicates(subset=[\"Name\", \"Adress\"])\n",
    "                    df.to_csv(f\"Data/Yellowpages_{date.today().strftime('%d-%m-%Y')}.csv\")\n",
    "                    print(f\"Iteration number: {index+1}/{str(int(len(searches))*8)}.\")\n",
    "                if index % 8 == 0:\n",
    "                    self.driver.close()\n",
    "                    time.sleep(20)\n",
    "                    self.driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n",
    "                    \n",
    "                index = index + 1\n",
    "                              \n",
    "        df.to_csv(f\"Data/Yellowpages_{date.today().strftime('%d-%m-%Y')}.csv\")\n",
    "        return df     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper_Maps:\n",
    "    def __init__(self):\n",
    "        self.driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n",
    "    def scrape_maps(self, url):\n",
    "        driver = self.driver\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(1)\n",
    "        driver.find_element_by_xpath(\"//button[contains(@class, 'widget-consent-button-later')]\").click()\n",
    "        driver.find_element_by_xpath(\"//span[contains(@class, 'button-next')]\").click()\n",
    "\n",
    "        results = driver.find_elements_by_class_name('section-result')\n",
    "        dataDict = {\"Name\": [], \"Phone\": []}\n",
    "        for result in results:\n",
    "            dataDict[\"Name\"].append(result.find_element_by_class_name(\"section-result-title\").text)\n",
    "            dataDict[\"Phone\"].append(result.find_element_by_class_name(\"section-result-phone-number\").text)\n",
    "        df = pd.DataFrame(dataDict)\n",
    "        return df\n",
    "\n",
    "    def scrape_maps_detailed(self, url, length_of_search = 100):\n",
    "        driver = self.driver\n",
    "        driver.implicitly_wait(0.5)\n",
    "        driver.get(url)\n",
    "        firstItemT0 = \"\"\n",
    "        i = 1\n",
    "        dataDict = {\"Name\": [], \"Phone\": [], \"Adress\": [], \"GeoLocation\": [], \"Website\": [], \"Category\": [], \"Crops\": [], \"Source\": [], \"DateUpdated\": [], \"DateChecked\": []}\n",
    "        while True:\n",
    "            try:\n",
    "                time.sleep(1.5)\n",
    "                results = driver.find_elements_by_class_name('section-result')\n",
    "                firstItemT1 = results[0].find_element_by_class_name(\"section-result-title\").text\n",
    "            except:\n",
    "                break\n",
    "            if i >= length_of_search:\n",
    "                break\n",
    "            if firstItemT1 != firstItemT0:\n",
    "                numResults = 0\n",
    "                for result in results:\n",
    "                    dataDict[\"Name\"].append(result.find_element_by_class_name(\"section-result-title\").text)\n",
    "                    dataDict[\"Phone\"].append(result.find_element_by_class_name(\"section-result-phone-number\").text)\n",
    "                    dataDict[\"Category\"].append(result.find_element_by_class_name(\"section-result-details\").text)\n",
    "                    dataDict[\"Source\"].append(url)\n",
    "                    dataDict[\"DateUpdated\"].append(date.today().strftime(\"%d/%m/%Y\"))\n",
    "                    dataDict[\"DateChecked\"].append(\"-\")\n",
    "                    dataDict[\"Crops\"].append(\"-\")\n",
    "                    dataDict[\"GeoLocation\"].append(\"-\")\n",
    "                    numResults = numResults + 1\n",
    "\n",
    "                for title in dataDict[\"Name\"][-numResults:]:\n",
    "                    try:\n",
    "                        #results = driver.find_elements_by_class_name('section-result-title')\n",
    "                        driver.find_element_by_xpath('//span[contains(text(), \"' + title + '\")]').click()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    try:\n",
    "                        dataDict[\"Adress\"].append(driver.find_element_by_xpath(\"//div[@data-section-id='ad']\").text)\n",
    "                    except:\n",
    "                        dataDict[\"Adress\"].append(\"N/V\")\n",
    "\n",
    "                    try:\n",
    "                        dataDict[\"Website\"].append(driver.find_element_by_xpath(\"//div[@data-section-id='ap']\").text)\n",
    "                    except:\n",
    "                        dataDict[\"Website\"].append(\"N/V\")\n",
    "                \n",
    "                    try:\n",
    "                        driver.find_element_by_class_name(\"section-back-to-list-button\").click()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                try:\n",
    "                    driver.find_element_by_xpath(\"//button[contains(@class, 'widget-consent-button-later')]\").click()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    driver.find_element_by_xpath(\"//span[contains(@class, 'button-next')]\").click()\n",
    "                    firstItemT0 = firstItemT1\n",
    "                    i = i + 1\n",
    "                except:\n",
    "                    break\n",
    "        df = pd.DataFrame(dataDict)\n",
    "        return df\n",
    "\n",
    "    def scrape_maps_multiple_searches(self, searches):\n",
    "        index = 1\n",
    "        # 111, 155\n",
    "        # -11, -43\n",
    "        #for xCOR in range(114, 119, 8):\n",
    "            #for yCOR in range(-22, -43, -4):\n",
    "                #location = f\"{str(yCOR)},{str(xCOR)},8.67z\"\n",
    "        xCOR = 117\n",
    "        yCOR = -30\n",
    "        location = f\"{str(yCOR)},{str(xCOR)},4.67z\"\n",
    "        for title in searches:\n",
    "            for state in [\"New South Wales\", \"Queensland\", \"Victoria\", \"Western Australia\", \"South Australia\", \"Tasmania\", \"Nothern Territory\"]:  \n",
    "                url = f\"https://www.google.com/maps/search/{title}+in+{state}/@{location}/data=!3m1!4b1?hl=en\"            \n",
    "                df_page = self.scrape_maps_detailed(url, 120)\n",
    "                if index == 1:  \n",
    "                    df = df_page\n",
    "                else:\n",
    "                    df = pd.concat([df, df_page], sort=False)\n",
    "                    if index % 12 == 0:\n",
    "                        df = df.drop_duplicates(subset=[\"Name\", \"Adress\"])\n",
    "                        df.to_csv(f\"Data/Googlemaps_{date.today().strftime('%d-%m-%Y')}.csv\")\n",
    "                        self.driver.close()\n",
    "                        time.sleep(3)\n",
    "                        self.driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n",
    "                index = index + 1\n",
    "        return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YP_Tool = Scraper_Yellowpages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPS_Tool = Scraper_Maps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = pd.read_excel(\"Inputs2.xlsx\", header=1)\n",
    "Searches = input_file[\"Search\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MAPS_Tool.scrape_maps_multiple_searches(Searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = YP_Tool.scrape_multiple_searches(Searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
