{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper_Yellowpages:\n",
    "    def __init__(self):\n",
    "        self.url = \"https://www.yellowpages.com.au/search/listings?clue=farm&locationClue=All+States&lat=&lon=&selectedViewMode=list\"\n",
    "        #self.driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"headless\")\n",
    "        self.driver = webdriver.Chrome(chrome_options=options)\n",
    "        self.driver = webdriver.Chrome()\n",
    "    def scrape_page(self, url = \"\"):\n",
    "        driver = self.driver\n",
    "        if url != \"\":\n",
    "            driver.get(url)\n",
    "        else:\n",
    "            url = driver.current_url\n",
    "\n",
    "        results = driver.find_elements_by_class_name('listing')\n",
    "\n",
    "        columns = [\"Name\", \"Adress\", \"Phone\", \"Website\", \"Email\", \"Source\", \"DateUpdated\", \"DateChecked\"]\n",
    "        webObjects = [\"listing-name\", \"listing-address\", \"contact-text\", \"contact-url\", \"contact-email\", 0, 0]\n",
    "        dataDict = {\"Name\": [], \"Adress\": [], \"Phone\": [], \"Website\": [], \"Email\": [], \"Source\": [], \"DateUpdated\": [], \"DateChecked\": []}\n",
    "\n",
    "        for result in results:\n",
    "            for i in range(0, len(columns)):\n",
    "                if columns[i] == \"DateUpdated\":\n",
    "                    dataDict[columns[i]].append(date.today().strftime(\"%d/%m/%Y\"))\n",
    "                elif columns[i] == \"Source\":\n",
    "                    dataDict[columns[i]].append(url)\n",
    "                elif columns[i] == \"DateChecked\":\n",
    "                    dataDict[columns[i]].append(\"-\")\n",
    "                elif columns[i] == \"Phone\":\n",
    "                    try:\n",
    "                        value = result.find_element_by_class_name(webObjects[i])\n",
    "                        dataDict[columns[i]].append(value.text)\n",
    "                    except:\n",
    "                        dataDict[columns[i]].append(\"N/V\")\n",
    "                elif columns[i] == \"Email\":\n",
    "                    try:\n",
    "                        value = result.find_element_by_class_name(webObjects[i])\n",
    "                        dataDict[columns[i]].append(value.get_attribute(\"data-email\"))\n",
    "                    except:\n",
    "                        dataDict[columns[i]].append(\"N/V\")\n",
    "                elif columns[i] == \"Website\":\n",
    "                    try:\n",
    "                        value = result.find_element_by_class_name(webObjects[i])\n",
    "                        dataDict[columns[i]].append(value.get_attribute(\"href\"))\n",
    "                    except:\n",
    "                        dataDict[columns[i]].append(\"N/V\")\n",
    "                elif webObjects[i] != 0:    \n",
    "                    try:\n",
    "                        value = result.find_element_by_class_name(webObjects[i])\n",
    "                        dataDict[columns[i]].append(value.text)\n",
    "                    except:\n",
    "                        dataDict[columns[i]].append(\"N/V\") \n",
    "                else:\n",
    "                    dataDict[columns[i]].append(\"N/V\") \n",
    "\n",
    "        df = pd.DataFrame(dataDict)\n",
    "        return df\n",
    "\n",
    "    def scrape_all_pages(self, url):\n",
    "        driver = self.driver\n",
    "        driver.get(url)\n",
    "\n",
    "        i = 0\n",
    "        while i <= 50:\n",
    "            df_page = self.scrape_page()\n",
    "\n",
    "            if i != 0:\n",
    "                df = pd.concat([df, df_page], sort=False)\n",
    "            else:\n",
    "                df = df_page\n",
    "                \n",
    "            try:\n",
    "                driver.find_element_by_xpath(\"//a[contains(text(),'Next')]\").click()\n",
    "            except:\n",
    "                break\n",
    "            i = i + 1\n",
    "        return df\n",
    "    \n",
    "    def scrape_multiple_searches(self, searches):\n",
    "        for index, title in enumerate(searches):\n",
    "            url = f\"https://www.yellowpages.com.au/search/listings?clue={title}&locationClue=All+States&lat=&lon=&selectedViewMode=list\"\n",
    "            \n",
    "            df_page = self.scrape_all_pages(url)\n",
    "            if index == 0:  \n",
    "                df = df_page\n",
    "            else:\n",
    "                df = pd.concat([df, df_page], sort=False)\n",
    "        df.to_csv(f\"Data/Yellowpages_{date.today().strftime('%d-%m-%Y')}.csv\")\n",
    "        return df     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper_Maps:\n",
    "    def __init__(self):\n",
    "        self.driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n",
    "    def scrape_maps(self, url):\n",
    "        driver = self.driver\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(1)\n",
    "        driver.find_element_by_xpath(\"//button[contains(@class, 'widget-consent-button-later')]\").click()\n",
    "        driver.find_element_by_xpath(\"//span[contains(@class, 'button-next')]\").click()\n",
    "\n",
    "        results = driver.find_elements_by_class_name('section-result')\n",
    "        dataDict = {\"Name\": [], \"Phone\": []}\n",
    "        for result in results:\n",
    "            dataDict[\"Name\"].append(result.find_element_by_class_name(\"section-result-title\").text)\n",
    "            dataDict[\"Phone\"].append(result.find_element_by_class_name(\"section-result-phone-number\").text)\n",
    "        df = pd.DataFrame(dataDict)\n",
    "        return df\n",
    "\n",
    "    def scrape_maps_detailed(self, url, length_of_search = 100):\n",
    "        driver = self.driver\n",
    "        driver.implicitly_wait(0.5)\n",
    "        driver.get(url)\n",
    "        firstItemT0 = \"\"\n",
    "        i = 1\n",
    "        dataDict = {\"Name\": [], \"Phone\": [], \"Adress\": [], \"Website\": [], \"Source\": [], \"DateUpdated\": [], \"DateChecked\": []}\n",
    "        while True:\n",
    "            results = driver.find_elements_by_class_name('section-result')\n",
    "            firstItemT1 = results[0].find_element_by_class_name(\"section-result-title\").text\n",
    "            if i >= length_of_search/20:\n",
    "                break\n",
    "            if firstItemT1 != firstItemT0:\n",
    "                for result in results:\n",
    "                    dataDict[\"Name\"].append(result.find_element_by_class_name(\"section-result-title\").text)\n",
    "                    dataDict[\"Phone\"].append(result.find_element_by_class_name(\"section-result-phone-number\").text)\n",
    "                    dataDict[\"Source\"].append(url)\n",
    "                    dataDict[\"DateUpdated\"].append(date.today().strftime(\"%d/%m/%Y\"))\n",
    "                    dataDict[\"DateChecked\"].append(\"-\")\n",
    "\n",
    "                for title in dataDict[\"Name\"][-20:]:\n",
    "                    try:\n",
    "                        results = driver.find_elements_by_class_name('section-result-title')\n",
    "                        driver.find_element_by_xpath('//span[contains(text(), \"' + title + '\")]').click()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    try:\n",
    "                        dataDict[\"Adress\"].append(driver.find_element_by_xpath(\"//div[@data-section-id='ad']\").text)\n",
    "                    except:\n",
    "                        dataDict[\"Adress\"].append(\"N/V\")\n",
    "\n",
    "                    try:\n",
    "                        dataDict[\"Website\"].append(driver.find_element_by_xpath(\"//div[@data-section-id='ap']\").text)\n",
    "                    except:\n",
    "                        dataDict[\"Website\"].append(\"N/V\")\n",
    "\n",
    "                    try:\n",
    "                        driver.find_element_by_class_name(\"section-back-to-list-button\").click()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                try:\n",
    "                    driver.find_element_by_xpath(\"//button[contains(@class, 'widget-consent-button-later')]\").click()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    driver.find_element_by_xpath(\"//span[contains(@class, 'button-next')]\").click()\n",
    "                    firstItemT0 = firstItemT1\n",
    "                    i = i + 1\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "        df = pd.DataFrame(dataDict)\n",
    "        return df\n",
    "\n",
    "    def scrape_maps_multiple_searches(self, searches, length_of_search, locations):\n",
    "        index = 1\n",
    "        for location in locations:\n",
    "            for title_index, title in enumerate(searches):\n",
    "                url = f\"https://www.google.com/maps/search/{title}/@{location}/data=!3m1!4b1?hl=en\"            \n",
    "                df_page = self.scrape_maps_detailed(url, length_of_search[title_index])\n",
    "                if index == 1:  \n",
    "                    df = df_page\n",
    "                else:\n",
    "                    df = pd.concat([df, df_page], sort=False)\n",
    "                    if index % 20 == 0:\n",
    "                        df.to_csv(f\"Data/Googlemaps_{date.today().strftime('%d-%m-%Y')}.csv\")\n",
    "                index = index + 1\n",
    "        return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximilianwitte/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: use options instead of chrome_options\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "YP_Tool = Scraper_Yellowpages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for [geckodriver v0.26.0 macos] driver in cache \n",
      "File found in cache by path [/Users/maximilianwitte/.wdm/drivers/geckodriver/v0.26.0/macos/geckodriver]\n"
     ]
    }
   ],
   "source": [
    "MAPS_Tool = Scraper_Maps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = pd.read_excel(\"Inputs.xlsx\", header=1)\n",
    "Searches = input_file[\"Search\"][:8]\n",
    "Length_Of_Search = input_file[\"Length_of_Search\"][:8]\n",
    "Locations = input_file[\"Location\"][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-335e993f5167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAPS_Tool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape_maps_multiple_searches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSearches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLength_Of_Search\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLocations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-cc60e9047fb0>\u001b[0m in \u001b[0;36mscrape_maps_multiple_searches\u001b[0;34m(self, searches, length_of_search, locations)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtitle_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://www.google.com/maps/search/{title}/@{location},8z/data=!3m1!4b1?hl=en\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mdf_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape_maps_detailed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_of_search\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-cc60e9047fb0>\u001b[0m in \u001b[0;36mscrape_maps_detailed\u001b[0;34m(self, url, length_of_search)\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    390\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arrays must all be same length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "df = MAPS_Tool.scrape_maps_multiple_searches(Searches, Length_Of_Search, Locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -22.8079219,113.3173924,6.3z\n",
       "1     -28.3637621,113.6617329,7.39z\n",
       "2      -30.907351,114.9286246,8.52z\n",
       "3      -32.5815756,115.429443,8.52z\n",
       "4     -33.9052434,115.2650522,8.52z\n",
       "5      -34.0202397,117.154941,7.89z\n",
       "6      -30.8924962,120.848444,6.42z\n",
       "7     -31.1453231,134.5666518,6.42z\n",
       "8     -35.3175626,138.6552691,7.86z\n",
       "9      -37.374922,141.6841697,8.03z\n",
       "10     -38.1478205,145.340854,9.16z\n",
       "11     -37.7111722,146.7674402,8.2z\n",
       "12    -36.6529314,147.5387489,7.55z\n",
       "13    -35.0148433,148.9734658,8.51z\n",
       "14        -33.881234,149.9353022,9z\n",
       "Name: Location, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
